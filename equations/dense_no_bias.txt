Two layers neural network without biases:

Let $x[B,n], W_1[k,n], x_1[B,k], x_2[B,k], W_2[m,k], y[B,m], L[\text{scalar}]$ satisfy the following equations:

\begin{align*}
x_1&= xW_1^T\\
x_2&=\mu(x_1) \text{ (ReLU)}\\
y&=x_2W_2^T\\
L&=F(y).
\end{align*}

Then, 
\begin{align*}
\hline
\dfrac{\partial L}{\partial y}&=\nabla F(y) \quad [B,m]\\
\hline
\dfrac{\partial L}{\partial W_2}&=\left[\dfrac{\partial L}{\partial y}\right]^T x_2\quad [m,k]\\
\dfrac{\partial L}{\partial x_2}&=\dfrac{\partial L}{\partial y} W_2 \quad [B,k]\\
\hline
\text{Relu has no weights}&\text{, no gradient for it.}\\
\dfrac{\partial L}{\partial x_1} &= \dfrac{\partial L}{\partial x_2}\odot\dfrac{\partial x_2}{\partial x_1}=\dfrac{\partial L}{\partial x_2}\odot (x_1>0)\quad [B,k]\\
\hline
\dfrac{\partial L}{\partial W_1}&=\left[\dfrac{\partial L}{\partial x_1}\right]^T x \quad [k,n]\\
\dfrac{\partial L}{\partial x}&=\dfrac{\partial L}{\partial x_1}W_1 \quad [B,n]\\
\hline
\end{align*}